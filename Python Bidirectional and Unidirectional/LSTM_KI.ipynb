{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvkr8gTkyFgK",
        "outputId": "615c01aa-29af-4193-d5d8-8e264f5e8df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 50000 rows\n",
            "Columns: ['review', 'sentiment']\n",
            "Stratified by: sentiment\n",
            "\n",
            "Saved:\n",
            " - /content/train.csv 40000\n",
            " - /content/val.csv 5000\n",
            " - /content/test.csv 5000\n",
            "\n",
            "Label dist:\n",
            "train: {'positive': 20000, 'negative': 20000}\n",
            "val  : {'negative': 2500, 'positive': 2500}\n",
            "test : {'negative': 2500, 'positive': 2500}\n"
          ]
        }
      ],
      "source": [
        "SINGLE_CSV = \"/content/IMDB_Dataset_clean.csv\"\n",
        "OUT_DIR    = \"/content\"\n",
        "SEED       = 42\n",
        "TRAIN_R, VAL_R, TEST_R = 0.80, 0.10, 0.10\n",
        "\n",
        "import os, json, numpy as np, pandas as pd\n",
        "assert os.path.exists(SINGLE_CSV), f\"File tidak ditemukan: {SINGLE_CSV}\"\n",
        "\n",
        "df = pd.read_csv(SINGLE_CSV, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
        "print(\"Loaded:\", len(df), \"rows\")\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "label_candidates = [\"label\", \"sentiment\", \"target\", \"class\", \"category\", \"polarity\"]\n",
        "lower_map = {c.lower(): c for c in df.columns}\n",
        "label_col = next((lower_map[c] for c in label_candidates if c in lower_map), None)\n",
        "stratified = label_col is not None and df[label_col].nunique(dropna=True) > 1\n",
        "print(\"Stratified by:\", label_col if stratified else \"None (random split)\")\n",
        "\n",
        "rng = np.random.RandomState(SEED)\n",
        "\n",
        "def split_indices_random(n, tr, vr):\n",
        "    perm = np.arange(n); rng.shuffle(perm)\n",
        "    n_tr = int(round(n*tr)); n_v = int(round(n*vr))\n",
        "    if n_tr + n_v > n: n_v = max(0, n - n_tr)\n",
        "    return perm[:n_tr], perm[n_tr:n_tr+n_v], perm[n_tr+n_v:]\n",
        "\n",
        "def split_indices_stratified(df, label_col, tr, vr):\n",
        "    train_idx, val_idx, test_idx = [], [], []\n",
        "    for _, sub in df.groupby(label_col, dropna=False):\n",
        "        idx = sub.index.to_numpy()\n",
        "        rng.shuffle(idx)\n",
        "        n = len(idx)\n",
        "        n_tr = int(round(n*tr)); n_v = int(round(n*vr))\n",
        "        if n_tr + n_v > n: n_v = max(0, n - n_tr)\n",
        "        train_idx.append(idx[:n_tr])\n",
        "        val_idx.append(idx[n_tr:n_tr+n_v])\n",
        "        test_idx.append(idx[n_tr+n_v:])\n",
        "    return (np.concatenate(train_idx), np.concatenate(val_idx), np.concatenate(test_idx))\n",
        "\n",
        "if stratified:\n",
        "    tr_idx, va_idx, te_idx = split_indices_stratified(df, label_col, TRAIN_R, VAL_R)\n",
        "else:\n",
        "    tr_idx, va_idx, te_idx = split_indices_random(len(df), TRAIN_R, VAL_R)\n",
        "\n",
        "\n",
        "os.makedirs(os.path.join(OUT_DIR, \"splits\"), exist_ok=True)\n",
        "with open(os.path.join(OUT_DIR, \"splits\", \"split_indices.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"train\": tr_idx.tolist(), \"val\": va_idx.tolist(), \"test\": te_idx.tolist()}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# tulis CSV\n",
        "df_train = df.loc[tr_idx].sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "df_val   = df.loc[va_idx].sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "df_test  = df.loc[te_idx].sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "train_path = os.path.join(OUT_DIR, \"train.csv\")\n",
        "val_path   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "test_path  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "df_train.to_csv(train_path, index=False, encoding=\"utf-8\")\n",
        "df_val.to_csv(val_path, index=False, encoding=\"utf-8\")\n",
        "df_test.to_csv(test_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" -\", train_path, len(df_train))\n",
        "print(\" -\", val_path,   len(df_val))\n",
        "print(\" -\", test_path,  len(df_test))\n",
        "if stratified:\n",
        "    print(\"\\nLabel dist:\")\n",
        "    print(\"train:\", df_train[label_col].value_counts(dropna=False).to_dict())\n",
        "    print(\"val  :\", df_val[label_col].value_counts(dropna=False).to_dict())\n",
        "    print(\"test :\", df_test[label_col].value_counts(dropna=False).to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_CSV  = \"/content/train.csv\"\n",
        "VAL_CSV    = \"/content/val.csv\"\n",
        "TEST_CSV   = \"/content/test.csv\"\n",
        "\n",
        "import csv, re, os, time, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    train: str = TRAIN_CSV\n",
        "    val: str   = VAL_CSV\n",
        "    test: str  = TEST_CSV\n",
        "    max_len: int=128; batch_size:int=64; epochs:int=10; lr:float=1e-3\n",
        "    emb_dim:int=128; hidden:int=128; num_layers:int=1; dropout:float=0.2\n",
        "    min_freq:int=5; seed:int=42; grad_clip:float=1.0\n",
        "cfg = Config()\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def set_seed(s:int):\n",
        "    random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    try: import numpy as np; np.random.seed(s)\n",
        "    except: pass\n",
        "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def detect_delimiter(path):\n",
        "    with open(path,\"r\",encoding=\"utf-8-sig\",newline=\"\") as f: first=f.readline()\n",
        "    return \";\" if first.count(\";\")>first.count(\",\") else \",\"\n",
        "def resolve_text_label_idx(headers):\n",
        "    ti, li = 0, 1 if len(headers)>1 else (0,0)\n",
        "    lower=[h.strip().lstrip(\"\\ufeff\").lower() for h in headers]\n",
        "    for i,n in enumerate(lower):\n",
        "        if n in (\"text\",\"review\",\"sentence\"): ti=i\n",
        "        if n in (\"label\",\"sentiment\",\"target\"): li=i\n",
        "    return ti, li\n",
        "def read_csv_flex(path):\n",
        "    delim=detect_delimiter(path); out=[]\n",
        "    with open(path,\"r\",encoding=\"utf-8-sig\",newline=\"\") as f:\n",
        "        reader=csv.reader(f,delimiter=delim); headers=next(reader,None)\n",
        "        assert headers, f\"Empty header: {path}\"\n",
        "        ti,li=resolve_text_label_idx(headers)\n",
        "        for rec in reader:\n",
        "            if not rec or len(rec)==1: continue\n",
        "            if li<len(rec) and ti<len(rec): txt,lab=rec[ti],rec[li]\n",
        "            else: lab=rec[-1]; txt=\",\".join(rec[:-1])\n",
        "            txt,lab=(txt or \"\").strip(),(lab or \"\").strip()\n",
        "            if txt and lab: out.append((txt,lab))\n",
        "    return out\n",
        "\n",
        "TOKEN_RE=re.compile(r\"[A-Za-z0-9]+\")\n",
        "class Vocab:\n",
        "    def __init__(self): self.itos=[\"<pad>\",\"<unk>\"]; self.stoi={w:i for i,w in enumerate(self.itos)}; self.unk_id=1\n",
        "    def add(self,t):\n",
        "        if t in self.stoi: return self.stoi[t]\n",
        "        self.stoi[t]=len(self.itos); self.itos.append(t); return self.stoi[t]\n",
        "    def get(self,t): return self.stoi.get(t,self.unk_id)\n",
        "    def __len__(self): return len(self.itos)\n",
        "def tok(s): return TOKEN_RE.findall(s.lower())\n",
        "def build_vocab(rows,min_freq):\n",
        "    from collections import Counter\n",
        "    cnt=Counter(); [cnt.update(tok(x)) for x,_ in rows]\n",
        "    v=Vocab()\n",
        "    for t,c in sorted(cnt.items(), key=lambda x:x[1]):\n",
        "        if c>=min_freq: v.add(t)\n",
        "    return v\n",
        "def build_labels(rows):\n",
        "    labs=sorted(set(l for _,l in rows)); return {lab:i for i,lab in enumerate(labs)}\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, rows, vocab, labmap, max_len):\n",
        "        self.data=[]\n",
        "        for txt,lab in rows:\n",
        "            ids=[vocab.get(t) for t in tok(txt)]\n",
        "            if len(ids)>max_len: ids=ids[:max_len]\n",
        "            self.data.append((ids,len(ids),labmap[lab]))\n",
        "        self.max_len=max_len\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self,i): return self.data[i]\n",
        "\n",
        "def collate_pad(batch,pad_id=0,max_len=None):\n",
        "    lens=[b[1] for b in batch]; ys=torch.tensor([b[2] for b in batch],dtype=torch.long)\n",
        "    if max_len is None: max_len=max(lens) if lens else 1\n",
        "    xs=[(ids+[pad_id]*(max_len-l)) for ids,l,_ in batch]\n",
        "    return torch.tensor(xs,dtype=torch.long), torch.tensor(lens), ys\n",
        "\n",
        "class BiLstmClf(nn.Module):\n",
        "    def __init__(self,vocab_size,emb,hidden,layers,num_classes,drop):\n",
        "        super().__init__()\n",
        "        self.embed=nn.Embedding(vocab_size,emb,padding_idx=0)\n",
        "        self.lstm=nn.LSTM(emb,hidden,num_layers=layers,batch_first=True,bidirectional=True)\n",
        "        self.fc=nn.Linear(hidden*2,num_classes); self.drop=nn.Dropout(drop)\n",
        "    def forward(self,x,lens,train=True):\n",
        "        emb=self.embed(x);\n",
        "        if train: emb=self.drop(emb)\n",
        "        out,_=self.lstm(emb) # (B,T,2H)\n",
        "        B,T,H2=out.shape\n",
        "        rng=torch.arange(T,device=out.device).unsqueeze(0)\n",
        "        mask=(rng<lens.unsqueeze(1)).float()\n",
        "        pooled=(out*mask.unsqueeze(-1)).sum(dim=1)/lens.clamp(min=1).unsqueeze(1).float()\n",
        "        if train: pooled=self.drop(pooled)\n",
        "        return self.fc(pooled)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model,loader,num_classes,device):\n",
        "    model.eval(); ce=nn.CrossEntropyLoss()\n",
        "    total_loss,total=0.0,0; conf=torch.zeros(num_classes,num_classes,dtype=torch.long,device=device)\n",
        "    for x,lens,y in loader:\n",
        "        x,lens,y=x.to(device),lens.to(device),y.to(device)\n",
        "        logits=model(x,lens,train=False); loss=ce(logits,y)\n",
        "        total_loss+=loss.item()*y.size(0); total+=y.size(0)\n",
        "        pred=logits.argmax(dim=-1); idx=y*num_classes+pred\n",
        "        binc=torch.bincount(idx,minlength=num_classes*num_classes)\n",
        "        conf+=binc.view(num_classes,num_classes)\n",
        "    tp=conf.diag().float(); fp=conf.sum(0).float()-tp; fn=conf.sum(1).float()-tp\n",
        "    prec=torch.where(tp+fp>0,tp/(tp+fp),torch.zeros_like(tp))\n",
        "    rec =torch.where(tp+fn>0,tp/(tp+fn),torch.zeros_like(tp))\n",
        "    f1  =torch.where(prec+rec>0,2*prec*rec/(prec+rec),torch.zeros_like(tp))\n",
        "    acc=(tp.sum()/conf.sum().clamp_min(1)).item()\n",
        "    return (total_loss/max(total,1), acc, prec.mean().item(), rec.mean().item(), f1.mean().item())\n",
        "\n",
        "def main():\n",
        "    print(\"Device:\", DEVICE, \"| Mode: BIDIRECTIONAL\")\n",
        "    t0=time.time()\n",
        "    tr=read_csv_flex(cfg.train); va=read_csv_flex(cfg.val); te=read_csv_flex(cfg.test)\n",
        "    assert tr and va and te, \"Pastikan train/val/test CSV ada & non-empty.\"\n",
        "    vocab=build_vocab(tr,cfg.min_freq); labmap=build_labels(tr); C=len(labmap)\n",
        "    print(f\"Vocab={len(vocab)} | Labels={labmap}\")\n",
        "    ds_tr=TextDataset(tr,vocab,labmap,cfg.max_len); ds_va=TextDataset(va,vocab,labmap,cfg.max_len); ds_te=TextDataset(te,vocab,labmap,cfg.max_len)\n",
        "    g=torch.Generator(); g.manual_seed(cfg.seed)\n",
        "    dl_tr=DataLoader(ds_tr,batch_size=cfg.batch_size,shuffle=True,collate_fn=lambda b:collate_pad(b,0,cfg.max_len),generator=g)\n",
        "    mkfix=lambda ds: DataLoader(ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=lambda b:collate_pad(b,0,cfg.max_len))\n",
        "    dl_va,dl_te=mkfix(ds_va),mkfix(ds_te)\n",
        "    model=BiLstmClf(len(vocab),cfg.emb_dim,cfg.hidden,cfg.num_layers,C,cfg.dropout).to(DEVICE)\n",
        "    opt=torch.optim.Adam(model.parameters(),lr=cfg.lr); ce=nn.CrossEntropyLoss()\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        t_ep=time.time(); t_tr=time.time()\n",
        "        model.train(); sum_loss=sum_acc=cnt=0\n",
        "        for x,lens,y in dl_tr:\n",
        "            x,lens,y=x.to(DEVICE),lens.to(DEVICE),y.to(DEVICE)\n",
        "            logits=model(x,lens,train=True); loss=ce(logits,y)\n",
        "            opt.zero_grad(set_to_none=True); loss.backward()\n",
        "            if cfg.grad_clip: nn.utils.clip_grad_norm_(model.parameters(),cfg.grad_clip)\n",
        "            opt.step()\n",
        "            with torch.no_grad(): acc=(logits.argmax(-1)==y).float().mean().item()\n",
        "            b=y.size(0); sum_loss+=loss.item()*b; sum_acc+=acc*b; cnt+=b\n",
        "        tr_s=time.time()-t_tr\n",
        "        v_loss,v_acc,p,r,f1=evaluate(model,dl_va,C,DEVICE); val_s=time.time()-t_ep-tr_s\n",
        "        print(f\"Epoch {ep:02d} | train loss {sum_loss/max(cnt,1):.4f} acc {sum_acc/max(cnt,1):.3f} | val loss {v_loss:.4f} acc {v_acc:.3f} P {p:.3f} R {r:.3f} F1 {f1:.3f} | time train {tr_s:.1f}s val {val_s:.1f}s total {time.time()-t_ep:.1f}s\")\n",
        "    t_test=time.time(); tl,ta,tp,trr,tf1=evaluate(model,dl_te,C,DEVICE)\n",
        "    print(f\"TEST | loss {tl:.4f} | acc {ta:.3f} | P {tp:.3f} R {trr:.3f} F1 {tf1:.3f} | time {time.time()-t_test:.1f}s\")\n",
        "    print(f\"TOTAL {time.time()-t0:.1f}s\")\n",
        "\n",
        "if __name__==\"__main__\": main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3s0ENGt44lk",
        "outputId": "4868bb7b-0023-4edb-bca9-f6d8391cc815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu | Mode: BIDIRECTIONAL\n",
            "Vocab=36148 | Labels={'negative': 0, 'positive': 1}\n",
            "Epoch 01 | train loss 0.4964 acc 0.750 | val loss 0.3699 acc 0.832 P 0.834 R 0.832 F1 0.832 | time train 193.7s val 6.8s total 200.5s\n",
            "Epoch 02 | train loss 0.3362 acc 0.852 | val loss 0.3373 acc 0.854 P 0.857 R 0.854 F1 0.854 | time train 190.8s val 6.4s total 197.2s\n",
            "Epoch 03 | train loss 0.2711 acc 0.887 | val loss 0.3230 acc 0.866 P 0.866 R 0.866 F1 0.866 | time train 189.4s val 6.7s total 196.1s\n",
            "Epoch 04 | train loss 0.2217 acc 0.910 | val loss 0.3225 acc 0.870 P 0.870 R 0.870 F1 0.870 | time train 189.3s val 6.4s total 195.7s\n",
            "Epoch 05 | train loss 0.1766 acc 0.930 | val loss 0.3428 acc 0.865 P 0.866 R 0.865 F1 0.865 | time train 191.4s val 6.7s total 198.1s\n",
            "Epoch 06 | train loss 0.1364 acc 0.948 | val loss 0.4400 acc 0.856 P 0.858 R 0.856 F1 0.856 | time train 190.2s val 6.4s total 196.6s\n",
            "Epoch 07 | train loss 0.1079 acc 0.959 | val loss 0.4443 acc 0.861 P 0.862 R 0.861 F1 0.861 | time train 189.1s val 6.7s total 195.9s\n",
            "Epoch 08 | train loss 0.0812 acc 0.970 | val loss 0.5042 acc 0.856 P 0.859 R 0.856 F1 0.856 | time train 188.2s val 6.7s total 194.9s\n",
            "Epoch 09 | train loss 0.0636 acc 0.977 | val loss 0.5206 acc 0.864 P 0.864 R 0.864 F1 0.864 | time train 189.3s val 6.4s total 195.6s\n",
            "Epoch 10 | train loss 0.0499 acc 0.981 | val loss 0.5993 acc 0.863 P 0.863 R 0.863 F1 0.863 | time train 191.2s val 6.3s total 197.5s\n",
            "TEST | loss 0.5918 | acc 0.863 | P 0.863 R 0.863 F1 0.863 | time 5.9s\n",
            "TOTAL 1982.2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_CSV  = \"/content/train.csv\"\n",
        "VAL_CSV    = \"/content/val.csv\"\n",
        "TEST_CSV   = \"/content/test.csv\"\n",
        "\n",
        "import csv, re, os, time, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    train: str = TRAIN_CSV\n",
        "    val: str   = VAL_CSV\n",
        "    test: str  = TEST_CSV\n",
        "    max_len: int=128; batch_size:int=64; epochs:int=10; lr:float=1e-3\n",
        "    emb_dim:int=128; hidden:int=128; num_layers:int=1; dropout:float=0.2\n",
        "    min_freq:int=5; seed:int=42; grad_clip:float=1.0\n",
        "cfg = Config()\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def set_seed(s:int):\n",
        "    random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    try: import numpy as np; np.random.seed(s)\n",
        "    except: pass\n",
        "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def detect_delimiter(path):\n",
        "    with open(path,\"r\",encoding=\"utf-8-sig\",newline=\"\") as f: first=f.readline()\n",
        "    return \";\" if first.count(\";\")>first.count(\",\") else \",\"\n",
        "\n",
        "def resolve_text_label_idx(headers):\n",
        "    ti, li = 0, 1 if len(headers)>1 else (0,0)\n",
        "    lower=[h.strip().lstrip(\"\\ufeff\").lower() for h in headers]\n",
        "    for i,n in enumerate(lower):\n",
        "        if n in (\"text\",\"review\",\"sentence\"): ti=i\n",
        "        if n in (\"label\",\"sentiment\",\"target\"): li=i\n",
        "    return ti, li\n",
        "\n",
        "def read_csv_flex(path):\n",
        "    delim=detect_delimiter(path); out=[]\n",
        "    with open(path,\"r\",encoding=\"utf-8-sig\",newline=\"\") as f:\n",
        "        reader=csv.reader(f,delimiter=delim); headers=next(reader,None)\n",
        "        assert headers, f\"Empty header: {path}\"\n",
        "        ti,li=resolve_text_label_idx(headers)\n",
        "        for rec in reader:\n",
        "            if not rec or len(rec)==1: continue\n",
        "            if li<len(rec) and ti<len(rec): txt,lab=rec[ti],rec[li]\n",
        "            else: lab=rec[-1]; txt=\",\".join(rec[:-1])\n",
        "            txt,lab=(txt or \"\").strip(),(lab or \"\").strip()\n",
        "            if txt and lab: out.append((txt,lab))\n",
        "    return out\n",
        "\n",
        "TOKEN_RE=re.compile(r\"[A-Za-z0-9]+\")\n",
        "class Vocab:\n",
        "    def __init__(self): self.itos=[\"<pad>\",\"<unk>\"]; self.stoi={w:i for i,w in enumerate(self.itos)}; self.unk_id=1\n",
        "    def add(self,t):\n",
        "        if t in self.stoi: return self.stoi[t]\n",
        "        self.stoi[t]=len(self.itos); self.itos.append(t); return self.stoi[t]\n",
        "    def get(self,t): return self.stoi.get(t,self.unk_id)\n",
        "    def __len__(self): return len(self.itos)\n",
        "def tok(s): return TOKEN_RE.findall(s.lower())\n",
        "def build_vocab(rows,min_freq):\n",
        "    from collections import Counter\n",
        "    cnt=Counter(); [cnt.update(tok(x)) for x,_ in rows]\n",
        "    v=Vocab()\n",
        "    for t,c in sorted(cnt.items(), key=lambda x:x[1]):\n",
        "        if c>=min_freq: v.add(t)\n",
        "    return v\n",
        "def build_labels(rows):\n",
        "    labs=sorted(set(l for _,l in rows)); return {lab:i for i,lab in enumerate(labs)}\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, rows, vocab, labmap, max_len):\n",
        "        self.data=[]\n",
        "        for txt,lab in rows:\n",
        "            ids=[vocab.get(t) for t in tok(txt)]\n",
        "            if len(ids)>max_len: ids=ids[:max_len]\n",
        "            self.data.append((ids,len(ids),labmap[lab]))\n",
        "        self.max_len=max_len\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self,i): return self.data[i]\n",
        "\n",
        "def collate_pad(batch,pad_id=0,max_len=None):\n",
        "    lens=[b[1] for b in batch]; ys=torch.tensor([b[2] for b in batch],dtype=torch.long)\n",
        "    if max_len is None: max_len=max(lens) if lens else 1\n",
        "    xs=[(ids+[pad_id]*(max_len-l)) for ids,l,_ in batch]\n",
        "    return torch.tensor(xs,dtype=torch.long), torch.tensor(lens), ys\n",
        "\n",
        "class UniLstmClf(nn.Module):\n",
        "    def __init__(self,vocab_size,emb,hidden,layers,num_classes,drop):\n",
        "        super().__init__()\n",
        "        self.embed=nn.Embedding(vocab_size,emb,padding_idx=0)\n",
        "        self.lstm=nn.LSTM(emb,hidden,num_layers=layers,batch_first=True,bidirectional=False)\n",
        "        self.fc=nn.Linear(hidden,num_classes); self.drop=nn.Dropout(drop)\n",
        "    def forward(self,x,lens,train=True):\n",
        "        emb=self.embed(x);\n",
        "        if train: emb=self.drop(emb)\n",
        "        out,_=self.lstm(emb) # (B,T,H)\n",
        "        B,T,H=out.shape\n",
        "        rng=torch.arange(T,device=out.device).unsqueeze(0)\n",
        "        mask=(rng<lens.unsqueeze(1)).float()\n",
        "        pooled=(out*mask.unsqueeze(-1)).sum(dim=1)/lens.clamp(min=1).unsqueeze(1).float()\n",
        "        if train: pooled=self.drop(pooled)\n",
        "        return self.fc(pooled)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model,loader,num_classes,device):\n",
        "    model.eval(); ce=nn.CrossEntropyLoss()\n",
        "    total_loss,total=0.0,0; conf=torch.zeros(num_classes,num_classes,dtype=torch.long,device=device)\n",
        "    for x,lens,y in loader:\n",
        "        x,lens,y=x.to(device),lens.to(device),y.to(device)\n",
        "        logits=model(x,lens,train=False); loss=ce(logits,y)\n",
        "        total_loss+=loss.item()*y.size(0); total+=y.size(0)\n",
        "        pred=logits.argmax(dim=-1); idx=y*num_classes+pred\n",
        "        binc=torch.bincount(idx,minlength=num_classes*num_classes)\n",
        "        conf+=binc.view(num_classes,num_classes)\n",
        "    tp=conf.diag().float(); fp=conf.sum(0).float()-tp; fn=conf.sum(1).float()-tp\n",
        "    prec=torch.where(tp+fp>0,tp/(tp+fp),torch.zeros_like(tp))\n",
        "    rec =torch.where(tp+fn>0,tp/(tp+fn),torch.zeros_like(tp))\n",
        "    f1  =torch.where(prec+rec>0,2*prec*rec/(prec+rec),torch.zeros_like(tp))\n",
        "    acc=(tp.sum()/conf.sum().clamp_min(1)).item()\n",
        "    return (total_loss/max(total,1), acc, prec.mean().item(), rec.mean().item(), f1.mean().item())\n",
        "\n",
        "def main():\n",
        "    print(\"Device:\", DEVICE, \"| Mode: UNIdirectional\")\n",
        "    t0=time.time()\n",
        "    tr=read_csv_flex(cfg.train); va=read_csv_flex(cfg.val); te=read_csv_flex(cfg.test)\n",
        "    assert tr and va and te, \"Pastikan train/val/test CSV ada & non-empty.\"\n",
        "    vocab=build_vocab(tr,cfg.min_freq); labmap=build_labels(tr); C=len(labmap)\n",
        "    print(f\"Vocab={len(vocab)} | Labels={labmap}\")\n",
        "    ds_tr=TextDataset(tr,vocab,labmap,cfg.max_len); ds_va=TextDataset(va,vocab,labmap,cfg.max_len); ds_te=TextDataset(te,vocab,labmap,cfg.max_len)\n",
        "    g=torch.Generator(); g.manual_seed(cfg.seed)\n",
        "    dl_tr=DataLoader(ds_tr,batch_size=cfg.batch_size,shuffle=True,collate_fn=lambda b:collate_pad(b,0,cfg.max_len),generator=g)\n",
        "    mkfix=lambda ds: DataLoader(ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=lambda b:collate_pad(b,0,cfg.max_len))\n",
        "    dl_va,dl_te=mkfix(ds_va),mkfix(ds_te)\n",
        "    model=UniLstmClf(len(vocab),cfg.emb_dim,cfg.hidden,cfg.num_layers,C,cfg.dropout).to(DEVICE)\n",
        "    opt=torch.optim.Adam(model.parameters(),lr=cfg.lr); ce=nn.CrossEntropyLoss()\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        t_ep=time.time(); t_tr=time.time()\n",
        "        model.train(); sum_loss=sum_acc=cnt=0\n",
        "        for x,lens,y in dl_tr:\n",
        "            x,lens,y=x.to(DEVICE),lens.to(DEVICE),y.to(DEVICE)\n",
        "            logits=model(x,lens,train=True); loss=ce(logits,y)\n",
        "            opt.zero_grad(set_to_none=True); loss.backward()\n",
        "            if cfg.grad_clip: nn.utils.clip_grad_norm_(model.parameters(),cfg.grad_clip)\n",
        "            opt.step()\n",
        "            with torch.no_grad(): acc=(logits.argmax(-1)==y).float().mean().item()\n",
        "            b=y.size(0); sum_loss+=loss.item()*b; sum_acc+=acc*b; cnt+=b\n",
        "        tr_s=time.time()-t_tr\n",
        "        v_loss,v_acc,p,r,f1=evaluate(model,dl_va,C,DEVICE); val_s=time.time()-t_ep-tr_s\n",
        "        print(f\"Epoch {ep:02d} | train loss {sum_loss/max(cnt,1):.4f} acc {sum_acc/max(cnt,1):.3f} | val loss {v_loss:.4f} acc {v_acc:.3f} P {p:.3f} R {r:.3f} F1 {f1:.3f} | time train {tr_s:.1f}s val {val_s:.1f}s total {time.time()-t_ep:.1f}s\")\n",
        "    t_test=time.time(); tl,ta,tp,trr,tf1=evaluate(model,dl_te,C,DEVICE)\n",
        "    print(f\"TEST | loss {tl:.4f} | acc {ta:.3f} | P {tp:.3f} R {trr:.3f} F1 {tf1:.3f} | time {time.time()-t_test:.1f}s\")\n",
        "    print(f\"TOTAL {time.time()-t0:.1f}s\")\n",
        "\n",
        "if __name__==\"__main__\": main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-eJJvI25COx",
        "outputId": "93b8af81-478c-4bf7-f25a-f33f918006c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu | Mode: UNIdirectional\n",
            "Vocab=36148 | Labels={'negative': 0, 'positive': 1}\n",
            "Epoch 01 | train loss 0.5086 acc 0.742 | val loss 0.3904 acc 0.825 P 0.826 R 0.825 F1 0.825 | time train 112.1s val 3.4s total 115.5s\n",
            "Epoch 02 | train loss 0.3447 acc 0.848 | val loss 0.3503 acc 0.844 P 0.852 R 0.844 F1 0.844 | time train 109.4s val 3.4s total 112.9s\n",
            "Epoch 03 | train loss 0.2774 acc 0.883 | val loss 0.3459 acc 0.853 P 0.854 R 0.853 F1 0.853 | time train 108.8s val 3.4s total 112.3s\n",
            "Epoch 04 | train loss 0.2310 acc 0.907 | val loss 0.3236 acc 0.864 P 0.864 R 0.864 F1 0.864 | time train 109.1s val 3.5s total 112.5s\n",
            "Epoch 05 | train loss 0.1919 acc 0.923 | val loss 0.3545 acc 0.866 P 0.867 R 0.866 F1 0.866 | time train 108.7s val 3.5s total 112.2s\n",
            "Epoch 06 | train loss 0.1543 acc 0.940 | val loss 0.4248 acc 0.861 P 0.864 R 0.861 F1 0.860 | time train 108.8s val 3.3s total 112.2s\n",
            "Epoch 07 | train loss 0.1267 acc 0.952 | val loss 0.4122 acc 0.866 P 0.867 R 0.866 F1 0.866 | time train 109.3s val 3.3s total 112.5s\n",
            "Epoch 08 | train loss 0.0966 acc 0.964 | val loss 0.4813 acc 0.862 P 0.862 R 0.862 F1 0.862 | time train 108.7s val 3.1s total 111.8s\n",
            "Epoch 09 | train loss 0.0794 acc 0.971 | val loss 0.5185 acc 0.864 P 0.864 R 0.864 F1 0.864 | time train 109.4s val 3.1s total 112.5s\n",
            "Epoch 10 | train loss 0.0631 acc 0.977 | val loss 0.6427 acc 0.861 P 0.862 R 0.861 F1 0.861 | time train 109.0s val 3.1s total 112.1s\n",
            "TEST | loss 0.6123 | acc 0.865 | P 0.866 R 0.865 F1 0.865 | time 3.4s\n",
            "TOTAL 1138.1s\n"
          ]
        }
      ]
    }
  ]
}